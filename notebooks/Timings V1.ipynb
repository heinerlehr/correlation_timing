{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55dccf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import debugpy\n",
    "debugpy.configure(subProcess=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bca47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "# Add parent directory to path (most common use case)\n",
    "cwd = Path(os.getcwd())\n",
    "if cwd.name == 'notebooks':\n",
    "    sys.path.append(os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5d330e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cwd.name == 'notebooks':\n",
    "    os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebe537f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from typing import Tuple\n",
    "from itertools import zip_longest\n",
    "\n",
    "import orjson\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import expon, weibull_min, lognorm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.axes import Axes\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f631ed8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdd8d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv() \n",
    "# Remove warnings\n",
    "warnings.filterwarnings(\"ignore\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeb6c79",
   "metadata": {},
   "source": [
    "# Trying to determine optimal timing to look back per correlation type\n",
    "\n",
    "Currently, when identifying correlations with a water anomaly, Marvin uses a fixed time window of 1h for poultry and 2h for pigs and tries to identify factors that are possibly correlated with the anomaly. There is now the ability to determine individual look-back windows per type of correlation. This notebook tries to assist in the determination of the optimal lookup-back time for Marvin per correlation (type).\n",
    "\n",
    "**Dataset:**  \n",
    "The dataset we use for the analysis has anomalies (per farm and shed) and their correlations. That is for each anomaly, we have as many entries as correlating factors Marvin has identified in the lookback window. \n",
    "\n",
    "**Hypothesis:**  \n",
    "Assuming that the correlating factor \"causes\" the anomaly, it stands to reason to assume that of the correlating factor is \"present\" during a time, a number of anomalies will be caused with this specific correlating factor. \n",
    "\n",
    "**Sub-hypothesis 1:**\n",
    "If we measure the time difference between an anomaly and all possible correlation factors, we should how frequent another anomaly with that correlating factor occurs in the past.\n",
    "\n",
    "**Sub-hypothesis 2:**\n",
    "If we measure the time difference for all correlating factors of an anomaly, we would see how far back the same correlating factor occurs in other anomalies.\n",
    "\n",
    "**Limitations:**\n",
    "- The animal type is not provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33bb09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESS_BY_CATEGORY = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330533cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRCDIR = Path(\"/mnt/c/Users/heine/Downloads/rainfall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4299046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "files  = [f for f in SRCDIR.iterdir() if f.suffix == '.json']\n",
    "df = pd.DataFrame() \n",
    "for f in files:\n",
    "    with open(f, 'r') as infile:\n",
    "        data = orjson.loads(infile.read())\n",
    "        t_df = pd.json_normalize(data)\n",
    "        t_df['LocalTime'] = pd.to_datetime(t_df['LocalTime']) # type:ignore\n",
    "        t_df = t_df[['AnomalyId', 'LocalTime', 'FarmId', 'FarmName', 'ShedId', 'ShedName','Correlation','Category']]\n",
    "        df = pd.concat([df, t_df], ignore_index=True)\n",
    "df = df.sort_values(by='LocalTime')\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37572ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Correlation'] = [corr.strip() for corr in df['Correlation']]\n",
    "correlations = df['Correlation'].sort_values().unique()\n",
    "categories = df['Category'].unique()\n",
    "logger.info(f\"Found {len(correlations)} unique correlations and {len(categories)} unique categories.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf215b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_correlations_by_pairs(correlations: list|np.ndarray) -> Tuple[int, list]:\n",
    "\n",
    "    pairs = []\n",
    "    singles = []\n",
    "\n",
    "    special_pairs = [('Sunset','Sunrise'),('Lights On', 'Lights Off')]\n",
    "\n",
    "    for correlation in correlations:\n",
    "        if correlation.endswith('Increased'):\n",
    "            cat_name = correlation.split('Increased')[0]\n",
    "            if f\"{cat_name}Decreased\" in correlations:\n",
    "                pairs.append(correlation)\n",
    "        elif correlation.endswith('Decreased'):\n",
    "            cat_name = correlation.split('Decreased')[0]\n",
    "            if f\"{cat_name}Increased\" in correlations:\n",
    "                pairs.append(correlation)\n",
    "        elif any([correlation in pair for pair in special_pairs]):\n",
    "            for pair in special_pairs:\n",
    "                if correlation in pair:\n",
    "                    other = pair[1] if correlation == pair[0] else pair[0]\n",
    "                    break\n",
    "            if other in correlations:\n",
    "                pairs.append(correlation)\n",
    "            else:\n",
    "                singles.append(correlation)\n",
    "        else:\n",
    "            singles.append(correlation)\n",
    "    \n",
    "    pairs.sort() # automatic ordering\n",
    "    singles.sort()\n",
    "\n",
    "    return len(pairs+singles), pairs + singles\n",
    "\n",
    "def order_correlations_by_category(correlations: list|np.ndarray, df:pd.DataFrame) -> Tuple[int, list]:\n",
    "\n",
    "    pairs = []\n",
    "    singles = []\n",
    "    categories = df['Category'].unique()\n",
    "    assert len(categories)==2\n",
    "\n",
    "    for correlation in correlations:\n",
    "            if len(df[df['Correlation']==correlation]['Category'].unique()) == 2:\n",
    "                 pairs.append(correlation)\n",
    "            else:\n",
    "                singles.append(correlation)\n",
    "    \n",
    "    pairs.sort() # automatic ordering\n",
    "    singles.sort()\n",
    "\n",
    "    return len(pairs)*2+len(singles),pairs + singles\n",
    "\n",
    "\n",
    "if PROCESS_BY_CATEGORY:\n",
    "    number_of_plots, correlations_ordered = order_correlations_by_category(correlations=correlations, df=df)\n",
    "else:\n",
    "    number_of_plots, correlations_ordered = order_correlations_by_pairs(correlations=correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce12c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "farmids = df['FarmId'].unique()\n",
    "shedids = df['ShedId'].unique()\n",
    "logger.info(f\"Found {len(farmids)} farms {len(shedids)} sheds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21deccdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_of_dataset = df['LocalTime'].min()\n",
    "max_lookback_length = 4 # hours\n",
    "earliest_time = start_of_dataset + pd.Timedelta(hours=max_lookback_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fa4f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalyids = df['AnomalyId'].unique()\n",
    "logger.info(f\"Found {len(anomalyids)} unique anomalies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf96406e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all unique anomalies for each shed at least 2h after earliest_time\n",
    "anomalies = df.groupby(by=['ShedId', 'AnomalyId', 'Category']).agg({'LocalTime':'min'}).reset_index()\n",
    "anomalies = anomalies[anomalies['LocalTime'] > earliest_time]\n",
    "logger.info(f\"Found {len(anomalies)} unique anomalies after {earliest_time}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab7201e",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b66666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now categorize delays into 15-min intervals\n",
    "interval_labels = [f\"{i*15}-{(i+1)*15}min\" for i in range(int(max_lookback_length * 60 / 15))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a81fa61",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd8d454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_times(x: np.ndarray) -> Tuple[str,dict]:\n",
    "\n",
    "    x = np.sort(x)\n",
    "\n",
    "    shape, loc, scale = stats.lognorm.fit(x, floc=0)\n",
    "\n",
    "    # Fit Weibull using scipy's weibull_min\n",
    "    c, loc_w, scale_w = stats.weibull_min.fit(x, floc=0)\n",
    "\n",
    "    # Fit exponential\n",
    "    loc_e, scale_e = stats.expon.fit(x, floc=0)\n",
    "\n",
    "    # Compute log-likelihoods\n",
    "    ll_logn  = np.sum(stats.lognorm.logpdf(x, shape, loc, scale))\n",
    "    ll_weib  = np.sum(stats.weibull_min.logpdf(x, c, loc_w, scale_w))\n",
    "    ll_exp   = np.sum(stats.expon.logpdf(x, loc_e, scale_e))\n",
    "\n",
    "    ret = {\n",
    "        'logn' : \n",
    "            { \n",
    "                'log-likelihood': ll_logn,\n",
    "                'shape': shape,\n",
    "                'loc': loc,\n",
    "                'scale': scale\n",
    "            },\n",
    "        'weib':\n",
    "            { \n",
    "                'log-likelihood': ll_weib,\n",
    "                'c': c,\n",
    "                'loc_w': loc_w,\n",
    "                'scale_w': scale_w\n",
    "            },\n",
    "        'exp'  : \n",
    "                { \n",
    "                'log-likelihood': ll_exp,\n",
    "                'loc_e': loc_e,\n",
    "                'scale_e': scale_e\n",
    "            }\n",
    "    }\n",
    "    maxvalue = max(ll_logn, ll_weib, ll_exp)\n",
    "    maxtype = 'logn'\n",
    "    for type,value in ret.items():\n",
    "        if value['log-likelihood'] == maxvalue:\n",
    "            maxtype = type\n",
    "            break\n",
    "\n",
    "    return maxtype, ret\n",
    "\n",
    "def determine_type(correlations: list, corr_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    ret = pd.DataFrame()\n",
    "    for i,correlation in enumerate(correlations):\n",
    "        delays = corr_data[corr_data['Correlation']==correlation]['Delay']\n",
    "        type, value_dict = fit_times(delays)\n",
    "        parameters = ['p1', 'p2', 'p3']\n",
    "        values = value_dict[type]\n",
    "        log_likelihood = values.pop('log-likelihood')\n",
    "        ps = dict(zip_longest(parameters, list(values.values()), fillvalue=None))\n",
    "\n",
    "        t_df = pd.DataFrame({\n",
    "            'Correlation': correlation,\n",
    "            'Type': type,\n",
    "            'Log-Likelihood': log_likelihood,\n",
    "            **ps\n",
    "        }, index=[i])\n",
    "        ret = pd.concat([ret, t_df], ignore_index=True)\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b521a509",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ultra-simple mixture distribution fitting\n",
    "Minimal dependencies, straightforward code\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def fit_mixture_simple(data, dist1='expon', dist2='weibull_min'):\n",
    "    \"\"\"\n",
    "    Fit a two-component mixture distribution\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : array\n",
    "        Your observations (e.g., time delays)\n",
    "    dist1 : str\n",
    "        Component 1: 'expon', 'weibull_min', or 'lognorm'\n",
    "    dist2 : str\n",
    "        Component 2: 'expon', 'weibull_min', or 'lognorm'\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict with fitted parameters and statistics\n",
    "    \"\"\"\n",
    "    \n",
    "    data = np.asarray(data, dtype=float)\n",
    "    data = data[~np.isnan(data)]  # Remove NaN values\n",
    "    \n",
    "    dists = {\n",
    "        'expon': expon,\n",
    "        'weibull_min': weibull_min,\n",
    "        'lognorm': lognorm,\n",
    "    }\n",
    "    \n",
    "    D1 = dists[dist1]\n",
    "    D2 = dists[dist2]\n",
    "    \n",
    "    # Fit individual distributions for starting guesses\n",
    "    p1 = D1.fit(data)\n",
    "    p2 = D2.fit(data)\n",
    "    \n",
    "    # Objective function: negative log-likelihood\n",
    "    def objective(params):\n",
    "        lam = params[0]\n",
    "        \n",
    "        # Ensure lambda is valid\n",
    "        if lam <= 0 or lam >= 1:\n",
    "            return 1e10\n",
    "        \n",
    "        # Extract parameters for each distribution\n",
    "        p1_est = params[1:len(p1)+1]\n",
    "        p2_est = params[len(p1)+1:]\n",
    "        \n",
    "        try:\n",
    "            # Evaluate mixture density\n",
    "            pdf1 = D1.pdf(data, *p1_est)\n",
    "            pdf2 = D2.pdf(data, *p2_est)\n",
    "            \n",
    "            # Clamp to avoid log(0)\n",
    "            mix = np.clip(lam * pdf1 + (1-lam) * pdf2, 1e-10, None)\n",
    "            \n",
    "            return -np.sum(np.log(mix))\n",
    "        except:\n",
    "            return 1e10\n",
    "    \n",
    "    # Initial guess\n",
    "    x0 = [0.5] + list(p1) + list(p2)\n",
    "    \n",
    "    # Optimize\n",
    "    res = minimize(objective, x0, method='Nelder-Mead',\n",
    "                   options={'maxiter': 5000})\n",
    "    \n",
    "    # Extract results\n",
    "    lam = np.clip(res.x[0], 0.01, 0.99)\n",
    "    p1_fit = res.x[1:len(p1)+1]\n",
    "    p2_fit = res.x[len(p1)+1:]\n",
    "    \n",
    "    # Calculate statistics\n",
    "    n_params = len(res.x)\n",
    "    nll = res.fun\n",
    "    aic = 2*nll + 2*n_params\n",
    "    bic = 2*nll + n_params*np.log(len(data))\n",
    "    \n",
    "    return {\n",
    "        'lambda': lam,\n",
    "        'params1': p1_fit,\n",
    "        'params2': p2_fit,\n",
    "        'dist1': dist1,\n",
    "        'dist2': dist2,\n",
    "        'nll': nll,\n",
    "        'aic': aic,\n",
    "        'bic': bic,\n",
    "        'n': len(data),\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_mixture(data, result, bins=40):\n",
    "    \"\"\"Plot data histogram with fitted mixture overlay\"\"\"\n",
    "    \n",
    "    D1 = {'expon': expon, 'weibull_min': weibull_min, 'lognorm': lognorm}[result['dist1']]\n",
    "    D2 = {'expon': expon, 'weibull_min': weibull_min, 'lognorm': lognorm}[result['dist2']]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    \n",
    "    # Histogram\n",
    "    ax.hist(data, bins=bins, density=True, alpha=0.5, color='gray', edgecolor='black')\n",
    "    \n",
    "    # Fitted curves\n",
    "    x = np.linspace(np.percentile(data, 0.5), np.percentile(data, 99.5), 300)\n",
    "    \n",
    "    pdf1 = D1.pdf(x, *result['params1'])\n",
    "    pdf2 = D2.pdf(x, *result['params2'])\n",
    "    \n",
    "    w1 = result['lambda']\n",
    "    w2 = 1 - result['lambda']\n",
    "    \n",
    "    ax.plot(x, w1*pdf1, 'r-', lw=2.5, label=f'{result[\"dist1\"]} ({w1:.1%})')\n",
    "    ax.plot(x, w2*pdf2, 'b-', lw=2.5, label=f'{result[\"dist2\"]} ({w2:.1%})')\n",
    "    ax.plot(x, w1*pdf1 + w2*pdf2, 'k--', lw=3, label='Mixture')\n",
    "    \n",
    "    ax.set_xlabel('Time Delay')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.set_title(f'{w1:.1%} {result[\"dist1\"]} + {w2:.1%} {result[\"dist2\"]} (BIC={result[\"bic\"]:.1f})')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "def print_results(result):\n",
    "    \"\"\"Print results nicely\"\"\"\n",
    "    logger.info(\"\\n\" + \"=\"*60)\n",
    "    logger.info(f\"MIXTURE: {result['lambda']:.1%} {result['dist1']} + {1-result['lambda']:.1%} {result['dist2']}\")\n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(f\"Î» (mixture weight):     {result['lambda']:.6f}\")\n",
    "    logger.info(f\"params1 ({result['dist1']}):        {result['params1']}\")\n",
    "    logger.info(f\"params2 ({result['dist2']}):        {result['params2']}\")\n",
    "    logger.info(f\"Negative LL:            {result['nll']:.2f}\")\n",
    "    logger.info(f\"AIC:                    {result['aic']:.2f}\")\n",
    "    logger.info(f\"BIC:                    {result['bic']:.2f}\")\n",
    "    logger.info(f\"N samples:              {result['n']}\")\n",
    "    logger.info(\"=\"*60 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd919d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def determine_type_by_category(correlations: list, corr_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    ret = pd.DataFrame()\n",
    "    categories = corr_data['Category'].unique()\n",
    "    assert len(categories) == 2\n",
    "    for i,correlation in enumerate(correlations):\n",
    "        for category in categories:\n",
    "            delays = corr_data[(corr_data['Correlation']==correlation) & (corr_data['Category']==category)]['Delay']\n",
    "            if delays.empty:\n",
    "                type = 'No data'\n",
    "                log_likelihood = None\n",
    "                ps = {'p1': None, 'p2': None, 'p3': None}\n",
    "            else:\n",
    "                type, value_dict = fit_times(delays)\n",
    "                parameters = ['p1', 'p2', 'p3']\n",
    "                values = value_dict[type]\n",
    "                log_likelihood = values.pop('log-likelihood')\n",
    "                ps = dict(zip_longest(parameters, list(values.values()), fillvalue=None))\n",
    "\n",
    "            t_df = pd.DataFrame({\n",
    "                'Correlation': correlation,\n",
    "                'Category': category,\n",
    "                'Type': type,\n",
    "                'Log-Likelihood': log_likelihood,\n",
    "                **ps\n",
    "            }, index=[i])\n",
    "            ret = pd.concat([ret, t_df], ignore_index=True)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba481ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_fit(delays:pd.Series):\n",
    "    best_bic = float('inf')\n",
    "    best_result = None\n",
    "    models = [\n",
    "        ('expon', 'weibull_min'),\n",
    "        ('expon', 'lognorm'),\n",
    "        ('weibull_min', 'lognorm'),\n",
    "    ]\n",
    "    for dist1, dist2 in models:\n",
    "        result = fit_mixture_simple(delays, dist1=dist1, dist2=dist2)\n",
    "        if result['bic'] < best_bic:\n",
    "            best_bic = result['bic']\n",
    "            best_result = result\n",
    "    return best_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0872ccab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "delays = merged[(merged['Correlation']=='CO2 Decreased') & (merged['Category']=='Decreased Water')]['Delay']\n",
    "result = find_best_fit(delays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a908c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_mixture(delays, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390c73f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_type_by_category_mixed(correlations: list, corr_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    ret = pd.DataFrame()\n",
    "    categories = corr_data['Category'].unique()\n",
    "    assert len(categories) == 2\n",
    "    for i,correlation in enumerate(correlations):\n",
    "        for category in categories:\n",
    "            delays = corr_data[(corr_data['Correlation']==correlation) & (corr_data['Category']==category)]['Delay']\n",
    "            if delays.empty:\n",
    "                type = 'No data'\n",
    "                log_likelihood = None\n",
    "                ps = {'p1': None, 'p2': None, 'p3': None}\n",
    "            else:\n",
    "                type, value_dict = fit_times(delays)\n",
    "                parameters = ['p1', 'p2', 'p3']\n",
    "                values = value_dict[type]\n",
    "                log_likelihood = values.pop('log-likelihood')\n",
    "                ps = dict(zip_longest(parameters, list(values.values()), fillvalue=None))\n",
    "\n",
    "            t_df = pd.DataFrame({\n",
    "                'Correlation': correlation,\n",
    "                'Category': category,\n",
    "                'Type': type,\n",
    "                'Log-Likelihood': log_likelihood,\n",
    "                **ps\n",
    "            }, index=[i])\n",
    "            ret = pd.concat([ret, t_df], ignore_index=True)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04c1b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_subset(ax: Axes, subset: pd.DataFrame, n_sample: int, corr: str, \n",
    "                types:pd.DataFrame|None=None, category:str|None=None, cumulative: bool=True) -> None:\n",
    "    if subset.empty:\n",
    "        ax.set_title(f'Correlation: {corr} {category} (No Data)')\n",
    "        ax.axis('off')  # Remove empty plot\n",
    "        return\n",
    "    # Calculate percentage and cumulative percentage\n",
    "    subset['Percent'] = 100 * subset['Count'] / subset['Count'].sum()\n",
    "    subset = subset.sort_values('DelayInterval')\n",
    "    subset['CumPercent'] = subset['Percent'].cumsum()\n",
    "    \n",
    "    # Assign colors: blue for first 90%, orange for rest\n",
    "    colors = ['#1f77b4' if cum <= 90 else '#ff7f0e' for cum in subset['CumPercent']]\n",
    "    \n",
    "    sns.barplot(data=subset, x='DelayInterval', y='Percent', ax=ax, palette=colors)\n",
    "    \n",
    "    if cumulative:\n",
    "        # Add cumulative line on secondary axis\n",
    "        ax2 = ax.twinx()\n",
    "        ax2.plot(range(len(subset)), subset['CumPercent'].values, color='darkred', linestyle='-', linewidth=2, label='Cumulative %') # type:ignore\n",
    "        ax2.axhline(y=90, color='darkred', linestyle='--', alpha=0.5, label='90% threshold')\n",
    "        ax2.set_ylabel('Cumulative %', color='darkred')\n",
    "        ax2.tick_params(axis='y', labelcolor='darkred')\n",
    "        ax2.set_ylim(0, 105)\n",
    "\n",
    "    # Plot distribution fit if available\n",
    "    if isinstance(types, pd.DataFrame) and not types.empty:\n",
    "        if 'Category' in types.columns:\n",
    "            type = types[(types['Correlation'] == corr) & (types['Category'] == category)].iloc[0]\n",
    "        else:\n",
    "            type = types[types['Correlation'] == corr].iloc[0]\n",
    "        match type['Type']:\n",
    "            case 'weib':\n",
    "                c, loc, scale = type['p1'], type['p2'], type['p3']\n",
    "                p1 = f\"{c:.2f}\"\n",
    "                p2 = f\"{scale:.2f}\"\n",
    "                dist = stats.weibull_min(c, loc, scale)\n",
    "            case 'logn':\n",
    "                shape, loc, scale = type['p1'], type['p2'], type['p3']\n",
    "                p1 = f\"{shape:.2f}\"\n",
    "                p2 = f\"{scale:.2f}\"\n",
    "                dist = stats.lognorm(shape, loc, scale)\n",
    "            case 'exp':\n",
    "                loc, scale = type['p1'], type['p2']\n",
    "                p1 = ''\n",
    "                p2 = f\"{scale:.2f}\"\n",
    "                dist = stats.expon(loc, scale)\n",
    "            case _:\n",
    "                p1, p2 = '', ''\n",
    "                dist = None\n",
    "        \n",
    "        if dist:\n",
    "            # Integrate PDF over each 15-min interval to get expected percentage\n",
    "            interval_percentages = []\n",
    "            for interval_start in range(0, max_lookback_length*60, 15):\n",
    "                interval_end = interval_start + 15\n",
    "                # CDF gives cumulative probability, so CDF(b) - CDF(a) gives probability in [a,b]\n",
    "                prob = dist.cdf(interval_end) - dist.cdf(interval_start)\n",
    "                percentage = prob * 100\n",
    "                interval_percentages.append(percentage)\n",
    "            \n",
    "            # Plot at categorical x positions (matching the bars)\n",
    "            legend_label = f\"{type['Type']}\\nshape: {p1}\\nscale: {p2}\"\n",
    "            ax.plot(range(len(interval_percentages)), interval_percentages, 'r-', linewidth=2, alpha=0.7, label=legend_label, marker='o')\n",
    "        else:\n",
    "            legend_label = \"Unknown distribution\"\n",
    "\n",
    "    title = f'{corr}'\n",
    "    if category is not None:\n",
    "        title += f', {category}'\n",
    "    if isinstance(types, pd.DataFrame) and not types.empty:\n",
    "        title += f' N={n_sample:,}'\n",
    "    ax.set_title(f'{corr} {category} N={n_sample:,}')\n",
    "    ax.set_xlabel('Delay Interval')\n",
    "    ax.set_ylabel('Percent')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    if isinstance(types, pd.DataFrame) and not types.empty:\n",
    "        ax.legend()\n",
    "\n",
    "def plot(number_of_plots:int, result: pd.DataFrame, df: pd.DataFrame, correlations:list,\n",
    "         types:pd.DataFrame|None=None, category:str|None=None, cumulative: bool=True) -> None:\n",
    "    rows = number_of_plots + 1\n",
    "    rows = math.ceil(rows/2)\n",
    "    columns = 2\n",
    "    fig, axs = plt.subplots(rows, columns, figsize=(20,rows*4+20))\n",
    "\n",
    "    # Add overall figure title\n",
    "    fig.suptitle(f'Correlation Delay Analysis\\n{len(anomalyids):,} anomalies from {len(farmids):,} farms and {len(shedids):,} sheds', \n",
    "                fontsize=16, fontweight='bold', y=1.01)\n",
    "\n",
    "    axs = axs.flatten() if rows > 1 else [axs]\n",
    "    j=0\n",
    "    for corr in correlations:\n",
    "        if PROCESS_BY_CATEGORY:\n",
    "            subset = result[(result['Correlation'] == corr) & (result['Category'] == categories[0])].copy()\n",
    "            n_sample = len(df[(df['Correlation']==corr) & (df['Category'] == categories[0])])\n",
    "            plot_subset(ax=axs[j], subset=subset, n_sample=n_sample, corr=corr, types=types, category=categories[0], cumulative=cumulative)\n",
    "            j += 1\n",
    "            subset = result[(result['Correlation'] == corr) & (result['Category'] == categories[1])].copy()\n",
    "            n_sample = len(df[(df['Correlation']==corr) & (df['Category'] == categories[1])])\n",
    "            plot_subset(ax=axs[j], subset=subset, n_sample=n_sample, corr=corr, types=types, category=categories[1], cumulative=cumulative)\n",
    "            j += 1\n",
    "        else:\n",
    "            n_sample = len(df[df['Correlation']==corr])\n",
    "            subset = result[result['Correlation'] == corr].copy()\n",
    "            plot_subset(ax=axs[j], subset=subset, n_sample=n_sample, corr=corr, types=types, cumulative=cumulative)\n",
    "            j += 1\n",
    "    \n",
    "    # Remove any remaining empty subplots\n",
    "    for k in range(j, len(axs)):\n",
    "        axs[k].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddf613b",
   "metadata": {},
   "source": [
    "## Sub-hypothesis 1:\n",
    "Measuring the time difference between an anomaly and all possible correlation factors, we should how frequent another anomaly with that correlating factor occurs in the past."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b528c737",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_HYPOTHESIS_1 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8c06e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_HYPOTHESIS_1:\n",
    "    if not PROCESS_BY_CATEGORY:\n",
    "        # Self-join approach\n",
    "        merged = anomalies.merge(df, on='ShedId', suffixes=('_anomaly', '_corr'))\n",
    "\n",
    "        # Filter: correlation must be before anomaly\n",
    "        merged = merged[merged['LocalTime_corr'] < merged['LocalTime_anomaly']]\n",
    "\n",
    "        # Calculate time difference\n",
    "        merged['Delay'] = (merged['LocalTime_anomaly'] - merged['LocalTime_corr']).dt.total_seconds() / 60\n",
    "\n",
    "        # Filter: only within lookback window\n",
    "        merged = merged[merged['Delay'] <= max_lookback_length * 60]\n",
    "\n",
    "        # Categorize and count\n",
    "        merged['DelayInterval'] = pd.cut(merged['Delay'], bins=range(0, max_lookback_length*60+1, 15), labels=interval_labels, right=False)\n",
    "        result = merged.groupby(['Correlation', 'DelayInterval']).size().reset_index(name='Count')\n",
    "    else:\n",
    "        # Process each category separately\n",
    "        results = []\n",
    "        mergeds = []\n",
    "        for category in categories:\n",
    "            anomalies_cat = anomalies[anomalies['Category'] == category]\n",
    "            df_cat = df[df['Category'] == category]\n",
    "\n",
    "            merged = anomalies_cat.merge(df_cat, on=['ShedId','Category'], suffixes=('_anomaly', '_corr'))\n",
    "\n",
    "            # Filter: correlation must be before anomaly\n",
    "            merged = merged[merged['LocalTime_corr'] < merged['LocalTime_anomaly']]\n",
    "\n",
    "\n",
    "            # Calculate time difference\n",
    "            merged['Delay'] = (merged['LocalTime_anomaly'] - merged['LocalTime_corr']).dt.total_seconds() / 60\n",
    "\n",
    "            # Filter: only within lookback window\n",
    "            merged = merged[merged['Delay'] <= max_lookback_length * 60]\n",
    "\n",
    "            # Categorize and count\n",
    "            merged['DelayInterval'] = pd.cut(merged['Delay'], bins=range(0, max_lookback_length*60+1, 15), labels=interval_labels, right=False)\n",
    "            mergeds.append(merged)\n",
    "            result_cat = merged.groupby(['Correlation', 'Category', 'DelayInterval']).size().reset_index(name='Count')\n",
    "            results.append(result_cat)\n",
    "        \n",
    "        merged = pd.concat(mergeds, ignore_index=True)\n",
    "        result = pd.concat(results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15db4bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_HYPOTHESIS_1:\n",
    "    if PROCESS_BY_CATEGORY:\n",
    "        types = determine_type_by_category(correlations=correlations_ordered, corr_data=merged)\n",
    "    else:\n",
    "        types = determine_type(correlations=correlations_ordered, corr_data=merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ee14aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_HYPOTHESIS_1:\n",
    "    # Alternative: with cumulative line\n",
    "    plot(number_of_plots=number_of_plots, result=result, df=df, correlations=correlations_ordered, types=types, cumulative=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95286cc0",
   "metadata": {},
   "source": [
    "## Sub-hypothesis 2:\n",
    "If we measure the time difference for all correlating factors of an anomaly, we would see how far back the same correlating factor occurs in other anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a91ea78",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_HYPOTHESIS_2 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa59d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timedifferences(df: pd.DataFrame, anomalies: pd.DataFrame, max_lookback_length: int) -> pd.DataFrame:\n",
    "    # Different approach: for each anomaly, only look at previous occurrences of its associated correlations\n",
    "    # For each anomaly-correlation pair, find previous occurrences of that SAME correlation type\n",
    "\n",
    "    # Start with the full dataset (each row is an anomaly-correlation pair)\n",
    "    result_list = []\n",
    "\n",
    "    for _, anomaly_row in anomalies.iterrows():\n",
    "        shed_id = anomaly_row['ShedId']\n",
    "        anomaly_id = anomaly_row['AnomalyId']\n",
    "        anomaly_time = anomaly_row['LocalTime']\n",
    "        \n",
    "        # Get all correlations associated with this specific anomaly\n",
    "        anomaly_correlations = df[(df['ShedId'] == shed_id) & (df['AnomalyId'] == anomaly_id)]['Correlation'].unique()\n",
    "        \n",
    "        # For each correlation type associated with this anomaly\n",
    "        for corr_type in anomaly_correlations:\n",
    "            # Find previous occurrences of this SAME correlation type in the same shed\n",
    "            previous_same_corr = df[\n",
    "                (df['ShedId'] == shed_id) & \n",
    "                (df['Correlation'] == corr_type) & \n",
    "                (df['LocalTime'] < anomaly_time)\n",
    "            ]\n",
    "            \n",
    "            # Calculate time differences\n",
    "            for _, prev_row in previous_same_corr.iterrows():\n",
    "                delay_minutes = (anomaly_time - prev_row['LocalTime']).total_seconds() / 60\n",
    "                \n",
    "                # Only include if within lookback window\n",
    "                if delay_minutes <= max_lookback_length * 60:\n",
    "                    result_list.append({\n",
    "                        'AnomalyId': anomaly_id,\n",
    "                        'ShedId': shed_id,\n",
    "                        'Correlation': corr_type,\n",
    "                        'Delay': delay_minutes\n",
    "                    })\n",
    "\n",
    "    return  pd.DataFrame(result_list)\n",
    "\n",
    "def get_timedifferences_per_category(df: pd.DataFrame, anomalies: pd.DataFrame, max_lookback_length: int) -> pd.DataFrame:\n",
    "    # Different approach: for each anomaly, only look at previous occurrences of its associated correlations\n",
    "    # For each anomaly-correlation pair, find previous occurrences of that SAME correlation type\n",
    "\n",
    "    # Start with the full dataset (each row is an anomaly-correlation pair)\n",
    "    result_list = []\n",
    "    categories = anomalies['Category'].unique()\n",
    "    assert len(categories) == 2\n",
    "\n",
    "    for _, anomaly_row in anomalies.iterrows():\n",
    "        shed_id = anomaly_row['ShedId']\n",
    "        anomaly_id = anomaly_row['AnomalyId']\n",
    "        anomaly_time = anomaly_row['LocalTime']\n",
    "        category = anomaly_row['Category']\n",
    "        \n",
    "        # Get all correlations associated with this specific anomaly\n",
    "        anomaly_correlations = df[(df['ShedId'] == shed_id) & (df['AnomalyId'] == anomaly_id) & (df['Category'] == category)]['Correlation'].unique()\n",
    "        \n",
    "        # For each correlation type associated with this anomaly\n",
    "        for corr_type in anomaly_correlations:\n",
    "            # Find previous occurrences of this SAME correlation type in the same shed\n",
    "            previous_same_corr = df[\n",
    "                (df['ShedId'] == shed_id) & \n",
    "                (df['Correlation'] == corr_type) & \n",
    "                (df['Category'] == category) &\n",
    "                (df['LocalTime'] < anomaly_time)\n",
    "            ]\n",
    "            \n",
    "            # Calculate time differences\n",
    "            for _, prev_row in previous_same_corr.iterrows():\n",
    "                delay_minutes = (anomaly_time - prev_row['LocalTime']).total_seconds() / 60\n",
    "                \n",
    "                # Only include if within lookback window\n",
    "                if delay_minutes <= max_lookback_length * 60:\n",
    "                    result_list.append({\n",
    "                        'AnomalyId': anomaly_id,\n",
    "                        'ShedId': shed_id,\n",
    "                        'Correlation': corr_type,\n",
    "                        'Category': category,\n",
    "                        'Delay': delay_minutes\n",
    "                    })\n",
    "\n",
    "    return pd.DataFrame(result_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6645b7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_HYPOTHESIS_2:\n",
    "    if PROCESS_BY_CATEGORY:\n",
    "        result2 = get_timedifferences_per_category(df, anomalies, max_lookback_length)\n",
    "            # Categorize and count\n",
    "        result2['DelayInterval'] = pd.cut(result2['Delay'], bins=range(0, max_lookback_length*60+1, 15), labels=interval_labels, right=False)\n",
    "        merged2 = result2.groupby(['Correlation', 'Category','DelayInterval']).size().reset_index(name='Count')\n",
    "    else:\n",
    "        result2 = get_timedifferences(df, anomalies, max_lookback_length)\n",
    "        # Categorize and count\n",
    "        result2['DelayInterval'] = pd.cut(result2['Delay'], bins=range(0, max_lookback_length*60+1, 15), labels=interval_labels, right=False)\n",
    "        merged2 = result2.groupby(['Correlation', 'DelayInterval']).size().reset_index(name='Count')\n",
    "    \n",
    "    logger.info(f\"Sub-hypothesis 2: Found {len(result2)} correlation occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c8ab25",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_HYPOTHESIS_2:\n",
    "    if PROCESS_BY_CATEGORY:\n",
    "        types = determine_type_by_category(correlations=correlations_ordered, corr_data=result2)\n",
    "    else:\n",
    "        types = determine_type(correlations=correlations_ordered, corr_data=result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30371a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_HYPOTHESIS_2:\n",
    "    # Alternative: with cumulative line\n",
    "    plot(number_of_plots=number_of_plots, result=merged2, df=df, correlations=correlations_ordered, types=types, cumulative=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "health_monitor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
